# write_utils.py
import pandas as pd
import numpy as np
import shutil
import os
import glob
from datetime import datetime
from config import *
from read_utils import safe_read_csv_files, invalidate_cache, CSV_FILES, CSV_DIR, ensure_csv_directory, \
    timing_decorator, timing_tracker


def get_required_columns(table_name):
    """è·å–æŒ‡å®šè¡¨å¿…éœ€çš„åˆ—"""
    required_columns = {
        'inventory': [
            "ID", "å•†å“ç¼–å·", "å…¥åº“æ—¶é—´", "å…¥åº“æ•°é‡", "å‡ºåº“æ€»æ•°é‡", "çŠ¶æ€",
            "åº“å­˜æ•°é‡", "åœ°å€ç±»å‹", "æ¥¼å±‚", "æ¶å·", "æ¡†å·", "åŒ…å·", "å•ä½"
        ],
        'manufacturer': ["ID", "inventory_id", "å‚å®¶è´§å·", "å‚å®¶", "å‚å®¶åœ°å€", "ç”µè¯"],
        'feature': [
            "ID", "inventory_id", "å•†å“ç±»å‹", "å•ä»·", "é‡é‡", "ç”¨é€”",
            "è§„æ ¼", "å¤‡æ³¨", "æè´¨", "é¢œè‰²", "å½¢çŠ¶", "é£æ ¼"
        ],
        'stock_capacity': ["æ¥¼å±‚", "æ¥¼å±‚å®¹é‡", "æ¥¼å±‚å‰©ä½™å®¹é‡"],
        'stock_out_records': ["ID", "inventory_id", "å‡ºåº“æ—¶é—´", "å‡ºåº“æ•°é‡", "æ“ä½œäººå‘˜", "å¤‡æ³¨"]
    }
    return required_columns.get(table_name, [])


def get_empty_dataframe_template(table_name):
    """è·å–æŒ‡å®šè¡¨çš„ç©ºDataFrameæ¨¡æ¿"""
    templates = {
        'inventory': pd.DataFrame(columns=[
            "ID", "å•†å“ç¼–å·", "å…¥åº“æ—¶é—´", "å…¥åº“æ•°é‡", "å‡ºåº“æ€»æ•°é‡", "çŠ¶æ€",
            "åº“å­˜æ•°é‡", "åœ°å€ç±»å‹", "æ¥¼å±‚", "æ¶å·", "æ¡†å·", "åŒ…å·", "å•ä½"
        ]),
        'manufacturer': pd.DataFrame(columns=[
            "ID", "inventory_id", "å‚å®¶è´§å·", "å‚å®¶", "å‚å®¶åœ°å€", "ç”µè¯"
        ]),
        'feature': pd.DataFrame(columns=[
            "ID", "inventory_id", "å•†å“ç±»å‹", "å•ä»·", "é‡é‡", "ç”¨é€”",
            "è§„æ ¼", "å¤‡æ³¨", "æè´¨", "é¢œè‰²", "å½¢çŠ¶", "é£æ ¼"
        ]),
        'stock_capacity': pd.DataFrame(columns=[
            "æ¥¼å±‚", "æ¥¼å±‚å®¹é‡", "æ¥¼å±‚å‰©ä½™å®¹é‡"
        ]),
        'stock_out_records': pd.DataFrame(columns=[
            "ID", "inventory_id", "å‡ºåº“æ—¶é—´", "å‡ºåº“æ•°é‡", "æ“ä½œäººå‘˜", "å¤‡æ³¨"
        ])
    }
    return templates.get(table_name, pd.DataFrame())


def get_unique_boxes_by_floor(floor, inventory_list):
    """è·å–æŒ‡å®šæ¥¼å±‚ä½¿ç”¨çš„å”¯ä¸€æ¡†å·é›†åˆ"""
    used_boxes = set()
    for item in inventory_list:
        item_floor = item.get("æ¥¼å±‚", 0)
        if item_floor == floor and "æ¡†å·" in item and item["æ¡†å·"] and not pd.isna(item["æ¡†å·"]):
            box_no = str(item["æ¡†å·"]).strip()
            if box_no:
                used_boxes.add(box_no)
    return used_boxes


def cleanup_old_backups():
    """æ¸…ç†æ—§çš„å¤‡ä»½æ–‡ä»¶ï¼Œåªä¿ç•™æœ€æ–°çš„å¤‡ä»½"""
    try:
        backup_pattern = os.path.join(CSV_DIR, "*.backup_*")
        backup_files = glob.glob(backup_pattern)

        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰é¢
        backup_files.sort(key=os.path.getmtime, reverse=True)

        # ä¸ºæ¯ä¸ªè¡¨ä¿ç•™æœ€æ–°çš„ä¸€ä¸ªå¤‡ä»½
        table_backups = {}
        for backup_file in backup_files:
            # æå–è¡¨åï¼ˆæ–‡ä»¶åå‰ç¼€ï¼‰
            filename = os.path.basename(backup_file)
            table_name = filename.split('.backup_')[0]

            if table_name not in table_backups:
                table_backups[table_name] = backup_file
            else:
                # åˆ é™¤å¤šä½™çš„å¤‡ä»½æ–‡ä»¶
                try:
                    os.remove(backup_file)
                    print(f"ğŸ—‘ï¸  åˆ é™¤æ—§å¤‡ä»½: {backup_file}")
                except Exception as e:
                    print(f"âš ï¸ åˆ é™¤å¤‡ä»½æ–‡ä»¶ {backup_file} å¤±è´¥: {str(e)}")

        print(f"âœ… å¤‡ä»½æ¸…ç†å®Œæˆï¼Œä¿ç•™ {len(table_backups)} ä¸ªæœ€æ–°å¤‡ä»½")

    except Exception as e:
        print(f"âš ï¸ æ¸…ç†å¤‡ä»½æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")


def create_single_backup():
    """ä¸ºæ‰€æœ‰è¡¨åˆ›å»ºå•ä¸€å¤‡ä»½ï¼ˆè¦†ç›–æ—§å¤‡ä»½ï¼‰"""
    try:
        ensure_csv_directory()
        backup_created = False

        for table, filepath in CSV_FILES.items():
            if os.path.exists(filepath):
                # ä½¿ç”¨å›ºå®šå¤‡ä»½æ–‡ä»¶åï¼Œè¦†ç›–æ—§çš„å¤‡ä»½
                backup_file = f"{filepath}.backup"
                try:
                    shutil.copy2(filepath, backup_file)
                    backup_created = True
                    print(f"ğŸ“¦ åˆ›å»ºå¤‡ä»½: {backup_file}")
                except Exception as e:
                    print(f"âš ï¸ åˆ›å»ºå¤‡ä»½æ–‡ä»¶ {backup_file} å¤±è´¥: {str(e)}")

        # æ¸…ç†å¯èƒ½å­˜åœ¨çš„å¸¦æ—¶é—´æˆ³çš„æ—§å¤‡ä»½
        cleanup_old_backups()

        return backup_created

    except Exception as e:
        print(f"âŒ åˆ›å»ºå¤‡ä»½å¤±è´¥: {str(e)}")
        return False


def restore_from_backup():
    """ä»å¤‡ä»½æ¢å¤æ•°æ®"""
    try:
        ensure_csv_directory()
        restored_tables = []

        for table, filepath in CSV_FILES.items():
            backup_file = f"{filepath}.backup"
            if os.path.exists(backup_file):
                try:
                    shutil.copy2(backup_file, filepath)
                    restored_tables.append(table)
                    print(f"ğŸ”„ ä»å¤‡ä»½æ¢å¤: {table}")
                except Exception as e:
                    print(f"âš ï¸ æ¢å¤ {table} å¤±è´¥: {str(e)}")

        if restored_tables:
            print(f"âœ… æˆåŠŸæ¢å¤ {len(restored_tables)} ä¸ªè¡¨çš„æ•°æ®")
            invalidate_cache()
            return True
        else:
            print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„å¤‡ä»½æ–‡ä»¶")
            return False

    except Exception as e:
        print(f"âŒ æ¢å¤å¤‡ä»½å¤±è´¥: {str(e)}")
        return False


@timing_decorator
def safe_write_csv_files(data):
    """å®‰å…¨åœ°å†™å…¥æ‰€æœ‰CSVæ–‡ä»¶ - ä½¿ç”¨å•ä¸€å¤‡ä»½ç­–ç•¥"""
    try:
        # ç¡®ä¿CSVç›®å½•å­˜åœ¨
        ensure_csv_directory()

        # é¦–å…ˆè¯»å–ç°æœ‰æ•°æ®ï¼Œç¡®ä¿ä¸ä¼šè¦†ç›–
        existing_data = safe_read_csv_files()

        # åˆå¹¶ç°æœ‰æ•°æ®å’Œæ–°æ•°æ®ï¼ˆä»¥æ–°æ•°æ®ä¸ºå‡†ï¼Œä½†ä¿ç•™ç°æœ‰æ•°æ®çš„å…¶ä»–è¡Œï¼‰
        merged_data = {}
        for table in REQUIRED_TABLES:
            if table in data and not data[table].empty:
                if table in existing_data and not existing_data[table].empty:
                    # å¦‚æœæœ‰IDåˆ—ï¼ŒåŸºäºIDåˆå¹¶ï¼Œå¦åˆ™ç›´æ¥ä½¿ç”¨æ–°æ•°æ®
                    if 'ID' in data[table].columns and 'ID' in existing_data[table].columns:
                        # è·å–æ–°æ•°æ®çš„IDé›†åˆ
                        new_ids = set(data[table]['ID'].astype(str))
                        # ä¿ç•™ç°æœ‰æ•°æ®ä¸­ä¸åœ¨æ–°æ•°æ®IDä¸­çš„è¡Œ
                        existing_filtered = existing_data[table][
                            ~existing_data[table]['ID'].astype(str).isin(new_ids)
                        ]
                        # åˆå¹¶æ•°æ®
                        merged_data[table] = pd.concat([existing_filtered, data[table]], ignore_index=True)
                    else:
                        # æ²¡æœ‰IDåˆ—ï¼Œç›´æ¥ä½¿ç”¨æ–°æ•°æ®
                        merged_data[table] = data[table]
                else:
                    # æ²¡æœ‰ç°æœ‰æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨æ–°æ•°æ®
                    merged_data[table] = data[table]
            else:
                # æ–°æ•°æ®ä¸ºç©ºï¼Œä½¿ç”¨ç°æœ‰æ•°æ®
                merged_data[table] = existing_data.get(table, pd.DataFrame())

        # åˆ›å»ºå•ä¸€å¤‡ä»½ï¼ˆè¦†ç›–æ—§å¤‡ä»½ï¼‰
        timing_tracker.start_operation("create_backup")
        backup_created = create_single_backup()
        timing_tracker.end_operation()

        if not backup_created:
            print("âš ï¸ å¤‡ä»½åˆ›å»ºå¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œå†™å…¥æ“ä½œ")

        # å†™å…¥CSVæ–‡ä»¶
        timing_tracker.start_operation("write_csv_files")
        write_success = True
        for table, filepath in CSV_FILES.items():
            try:
                df = merged_data.get(table, pd.DataFrame())
                if not df.empty:
                    # ç¡®ä¿ç›®å½•å­˜åœ¨
                    os.makedirs(os.path.dirname(filepath), exist_ok=True)
                    df.to_csv(filepath, index=False, encoding='utf-8-sig')
                    print(f"ğŸ’¾ æˆåŠŸå†™å…¥ {table}ï¼Œè¡Œæ•°: {len(df)}")
                else:
                    # å¦‚æœæ•°æ®ä¸ºç©ºï¼Œåˆ›å»ºç©ºçš„CSVæ–‡ä»¶ï¼ˆåªæœ‰è¡¨å¤´ï¼‰
                    df_template = get_empty_dataframe_template(table)
                    df_template.to_csv(filepath, index=False, encoding='utf-8-sig')
                    print(f"ğŸ“„ åˆ›å»ºç©ºæ–‡ä»¶: {table}")
            except Exception as e:
                print(f"âŒ å†™å…¥ {table} æ–‡ä»¶å¤±è´¥: {str(e)}")
                write_success = False

        timing_tracker.end_operation()

        if write_success:
            print("âœ… æ‰€æœ‰CSVæ–‡ä»¶å†™å…¥æˆåŠŸ")
            invalidate_cache()
            return True
        else:
            print("âŒ éƒ¨åˆ†æ–‡ä»¶å†™å…¥å¤±è´¥ï¼Œå°è¯•ä»å¤‡ä»½æ¢å¤")
            # å†™å…¥å¤±è´¥æ—¶å°è¯•æ¢å¤å¤‡ä»½
            restore_from_backup()
            return False

    except Exception as e:
        print(f"âŒ å†™å…¥CSVæ–‡ä»¶å¤±è´¥: {str(e)}")
        # å‘ç”Ÿå¼‚å¸¸æ—¶å°è¯•æ¢å¤å¤‡ä»½
        restore_from_backup()
        return False


@timing_decorator
def write_csv_data(data):
    """å†™å…¥CSVæ•°æ® - å®‰å…¨çš„å†™å…¥æµç¨‹"""
    return safe_write_csv_files(data)


@timing_decorator
def add_data_to_csv(new_data_dict):
    """
    å®‰å…¨åœ°æ·»åŠ æ•°æ®åˆ°CSVæ–‡ä»¶
    new_data_dict: {'table_name': DataFrameæˆ–å­—å…¸åˆ—è¡¨, ...}
    """
    try:
        from read_utils import read_csv_data

        # 1. é¦–å…ˆè¯»å–ç°æœ‰æ•°æ®
        existing_data = read_csv_data()

        # 2. åˆå¹¶æ–°æ•°æ®åˆ°ç°æœ‰æ•°æ®
        for table_name, new_data in new_data_dict.items():
            if table_name in existing_data:
                if isinstance(new_data, pd.DataFrame):
                    new_df = new_data
                else:
                    # å¦‚æœæ˜¯å­—å…¸åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºDataFrame
                    new_df = pd.DataFrame(new_data)

                if not new_df.empty:
                    # åˆå¹¶æ•°æ®
                    if not existing_data[table_name].empty:
                        existing_data[table_name] = pd.concat(
                            [existing_data[table_name], new_df],
                            ignore_index=True
                        )
                    else:
                        existing_data[table_name] = new_df

                    print(f"âœ… æˆåŠŸæ·»åŠ  {len(new_df)} è¡Œæ•°æ®åˆ° {table_name}")

        # 3. å†™å…¥åˆå¹¶åçš„æ•°æ®
        result = write_csv_data(existing_data)
        if result:
            print("ğŸ‰ æ•°æ®æ·»åŠ æˆåŠŸ")
        else:
            print("âŒ æ•°æ®æ·»åŠ å¤±è´¥")
        return result

    except Exception as e:
        print(f"âŒ æ·»åŠ æ•°æ®å¤±è´¥: {str(e)}")
        return False


@timing_decorator
def write_csv_data_optimized(csv_data):
    """ä¼˜åŒ–çš„æ•°æ®å†™å…¥å‡½æ•°"""
    try:
        from read_utils import _data_cache, _cache_timestamp
        _data_cache = {}
        _cache_timestamp = None
        return write_csv_data(csv_data)
    except Exception as e:
        print(f"âŒ æ•°æ®å†™å…¥å¤±è´¥: {str(e)}")
        return False


def list_backups():
    """åˆ—å‡ºæ‰€æœ‰å¤‡ä»½æ–‡ä»¶"""
    try:
        backup_pattern = os.path.join(CSV_DIR, "*.backup*")
        backup_files = glob.glob(backup_pattern)

        if backup_files:
            print("ğŸ“‹ ç°æœ‰å¤‡ä»½æ–‡ä»¶:")
            for backup_file in backup_files:
                file_size = os.path.getsize(backup_file)
                mod_time = datetime.fromtimestamp(os.path.getmtime(backup_file))
                print(f"  {os.path.basename(backup_file)}")
                print(f"    å¤§å°: {file_size} å­—èŠ‚, ä¿®æ”¹æ—¶é—´: {mod_time}")
        else:
            print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°å¤‡ä»½æ–‡ä»¶")

        return backup_files
    except Exception as e:
        print(f"âŒ åˆ—å‡ºå¤‡ä»½æ–‡ä»¶å¤±è´¥: {str(e)}")
        return []


def cleanup_all_backups():
    """æ¸…ç†æ‰€æœ‰å¤‡ä»½æ–‡ä»¶"""
    try:
        backup_files = list_backups()
        if not backup_files:
            print("â„¹ï¸  æ²¡æœ‰å¤‡ä»½æ–‡ä»¶éœ€è¦æ¸…ç†")
            return True

        confirm = input("âš ï¸  ç¡®å®šè¦åˆ é™¤æ‰€æœ‰å¤‡ä»½æ–‡ä»¶å—ï¼Ÿ(y/N): ")
        if confirm.lower() == 'y':
            deleted_count = 0
            for backup_file in backup_files:
                try:
                    os.remove(backup_file)
                    deleted_count += 1
                    print(f"ğŸ—‘ï¸  åˆ é™¤: {backup_file}")
                except Exception as e:
                    print(f"âš ï¸ åˆ é™¤ {backup_file} å¤±è´¥: {str(e)}")

            print(f"âœ… å·²åˆ é™¤ {deleted_count} ä¸ªå¤‡ä»½æ–‡ä»¶")
            return deleted_count == len(backup_files)
        else:
            print("âŒ æ“ä½œå·²å–æ¶ˆ")
            return False

    except Exception as e:
        print(f"âŒ æ¸…ç†å¤‡ä»½å¤±è´¥: {str(e)}")
        return False


# ------------------- å…¼å®¹æ€§å‡½æ•° -------------------
def write_excel_data(data):
    """å…¼å®¹æ€§å‡½æ•°ï¼Œä¿æŒåŸæœ‰æ¥å£"""
    return write_csv_data(data)


def safe_write_excel_file(data):
    """å…¼å®¹æ€§å‡½æ•°ï¼Œä¿æŒåŸæœ‰æ¥å£"""
    return safe_write_csv_files(data)