import os
import tempfile
import shutil
import pandas as pd
import numpy as np
from datetime import datetime
from functools import lru_cache
import glob

# æ³¨æ„ï¼šç¡®ä¿config.pyä¸­å®šä¹‰äº† REQUIRED_TABLESã€CACHE_TIMEOUTã€FLOOR_CAPACITY
# ç¤ºä¾‹config.pyé…ç½®ï¼ˆå¯æ ¹æ®å®é™…è°ƒæ•´ï¼‰ï¼š
# REQUIRED_TABLES = ['product', 'feature', 'inventory', 'location', 'operation_record', 'manufacturer', 'capacity']
# CACHE_TIMEOUT = 60  # ç¼“å­˜è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
# FLOOR_CAPACITY = 100  # æ¥¼å±‚å®¹é‡
from config import *


# ------------------- CSVæ–‡ä»¶è·¯å¾„å®šä¹‰ -------------------
CSV_DIR = 'csv'
CSV_FILES = {
    'product': os.path.join(CSV_DIR, 'product.csv'),  # å•†å“è¡¨
    'feature': os.path.join(CSV_DIR, 'feature.csv'),  # å•†å“ç‰¹å¾è¡¨
    'inventory': os.path.join(CSV_DIR, 'inventory.csv'),  # åº“å­˜è¡¨
    'location': os.path.join(CSV_DIR, 'location.csv'),  # ä½ç½®è¡¨
    'operation_record': os.path.join(CSV_DIR, 'operation_record.csv'),  # æ“ä½œè®°å½•è¡¨
    'manufacturer': os.path.join(CSV_DIR, 'manufacturer.csv'),  # å‚å®¶è¡¨
    'capacity': os.path.join(CSV_DIR, 'capacity.csv')  # å®¹é‡è¡¨
}


# ------------------- ç›®å½•æ“ä½œå‡½æ•° -------------------
def ensure_csv_directory():
    """ç¡®ä¿CSVç›®å½•å­˜åœ¨"""
    if not os.path.exists(CSV_DIR):
        os.makedirs(CSV_DIR)
        print(f"åˆ›å»ºCSVç›®å½•: {CSV_DIR}")


# ------------------- å¤‡ä»½ç®¡ç†å‡½æ•° -------------------
def cleanup_old_backups():
    """æ¸…ç†æ—§çš„å¤‡ä»½æ–‡ä»¶ï¼Œåªä¿ç•™æœ€æ–°çš„å¤‡ä»½"""
    try:
        backup_pattern = os.path.join(CSV_DIR, "*.backup_*")
        backup_files = glob.glob(backup_pattern)

        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰é¢
        backup_files.sort(key=os.path.getmtime, reverse=True)

        # ä¸ºæ¯ä¸ªè¡¨ä¿ç•™æœ€æ–°çš„ä¸€ä¸ªå¤‡ä»½
        table_backups = {}
        for backup_file in backup_files:
            # æå–è¡¨åï¼ˆæ–‡ä»¶åå‰ç¼€ï¼‰
            filename = os.path.basename(backup_file)
            table_name = filename.split('.backup_')[0]

            if table_name not in table_backups:
                table_backups[table_name] = backup_file
            else:
                # åˆ é™¤å¤šä½™çš„å¤‡ä»½æ–‡ä»¶
                try:
                    os.remove(backup_file)
                    print(f"ğŸ—‘ï¸  åˆ é™¤æ—§å¤‡ä»½: {backup_file}")
                except Exception as e:
                    print(f"âš ï¸ åˆ é™¤å¤‡ä»½æ–‡ä»¶ {backup_file} å¤±è´¥: {str(e)}")

        print(f"âœ… å¤‡ä»½æ¸…ç†å®Œæˆï¼Œä¿ç•™ {len(table_backups)} ä¸ªæœ€æ–°å¤‡ä»½")

    except Exception as e:
        print(f"âš ï¸ æ¸…ç†å¤‡ä»½æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")


def create_single_backup():
    """ä¸ºæ‰€æœ‰è¡¨åˆ›å»ºå•ä¸€å¤‡ä»½ï¼ˆè¦†ç›–æ—§å¤‡ä»½ï¼‰"""
    try:
        ensure_csv_directory()
        backup_created = False

        for table, filepath in CSV_FILES.items():
            if os.path.exists(filepath):
                # ä½¿ç”¨å›ºå®šå¤‡ä»½æ–‡ä»¶åï¼Œè¦†ç›–æ—§çš„å¤‡ä»½
                backup_file = f"{filepath}.backup"
                try:
                    # å¤åˆ¶æ–‡ä»¶æ—¶ä¿ç•™å…ƒæ•°æ®ï¼Œç¡®ä¿å¤‡ä»½å®Œæ•´
                    shutil.copy2(filepath, backup_file)
                    backup_created = True
                    print(f"ğŸ“¦ åˆ›å»ºå¤‡ä»½: {backup_file}")
                except Exception as e:
                    print(f"âš ï¸ åˆ›å»ºå¤‡ä»½æ–‡ä»¶ {backup_file} å¤±è´¥: {str(e)}")

        # æ¸…ç†å¯èƒ½å­˜åœ¨çš„å¸¦æ—¶é—´æˆ³çš„æ—§å¤‡ä»½
        cleanup_old_backups()

        return backup_created

    except Exception as e:
        print(f"âŒ åˆ›å»ºå¤‡ä»½å¤±è´¥: {str(e)}")
        return False


def restore_from_backup():
    """ä»å¤‡ä»½æ¢å¤æ•°æ®"""
    try:
        ensure_csv_directory()
        restored_tables = []

        for table, filepath in CSV_FILES.items():
            backup_file = f"{filepath}.backup"
            if os.path.exists(backup_file):
                try:
                    # å…ˆåˆ é™¤æŸåçš„æ–‡ä»¶ï¼Œå†æ¢å¤å¤‡ä»½
                    if os.path.exists(filepath):
                        os.remove(filepath)
                    shutil.copy2(backup_file, filepath)
                    restored_tables.append(table)
                    print(f"ğŸ”„ ä»å¤‡ä»½æ¢å¤: {table}")
                except Exception as e:
                    print(f"âš ï¸ æ¢å¤ {table} å¤±è´¥: {str(e)}")

        if restored_tables:
            print(f"âœ… æˆåŠŸæ¢å¤ {len(restored_tables)} ä¸ªè¡¨çš„æ•°æ®")
            invalidate_cache()
            return True
        else:
            print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„å¤‡ä»½æ–‡ä»¶")
            return False

    except Exception as e:
        print(f"âŒ æ¢å¤å¤‡ä»½å¤±è´¥: {str(e)}")
        return False


# ------------------- æ ¸å¿ƒä¼˜åŒ–ï¼šç¼“å­˜æœºåˆ¶ -------------------
@lru_cache(maxsize=1)
def get_cached_csv_data():
    """ç¼“å­˜CSVæ•°æ®ï¼Œ1åˆ†é’Ÿè‡ªåŠ¨å¤±æ•ˆ"""
    data = safe_read_csv_files()
    for table in REQUIRED_TABLES:
        if table not in data:
            data[table] = pd.DataFrame()
        # ä¿®å¤ï¼šç¡®ä¿IDåˆ—ç±»å‹ç»Ÿä¸€ä¸ºæ•´æ•°ï¼Œé¿å…åŒ¹é…é”™è¯¯
        data[table] = normalize_id_columns(data[table], table)
    return data


def invalidate_cache():
    """å¤±æ•ˆç¼“å­˜ï¼ˆæ•°æ®æ›´æ–°æ—¶è°ƒç”¨ï¼‰"""
    get_cached_csv_data.cache_clear()
    # é¢å¤–æ¸…ç†è‡ªå®šä¹‰ç¼“å­˜
    global _data_cache, _cache_timestamp
    _data_cache = {}
    _cache_timestamp = None


def normalize_id_columns(df, table_name):
    """æ ‡å‡†åŒ–IDåˆ—ç±»å‹ä¸ºæ•´æ•°ï¼Œé¿å…å­—ç¬¦ä¸²/æ•°å€¼æ··ç”¨å¯¼è‡´çš„åŒ¹é…é”™è¯¯"""
    if df.empty:
        return df

    id_column = get_id_column(table_name)
    if id_column and id_column in df.columns:
        # æ¸…ç†ç©ºå€¼/éæ•°å€¼ï¼Œè½¬ä¸ºæ•´æ•°
        df[id_column] = pd.to_numeric(df[id_column], errors='coerce').fillna(-1).astype(int)
        # è¿‡æ»¤æ— æ•ˆIDï¼ˆ-1ï¼‰
        df = df[df[id_column] != -1].reset_index(drop=True)

    # å¤„ç†å…³è”IDåˆ—
    related_id_cols = {
        'inventory': ['å…³è”å•†å“ç‰¹å¾ID', 'å…³è”ä½ç½®ID', 'å…³è”å‚å®¶ID'],
        'feature': ['å…³è”å•†å“ID'],
        'operation_record': ['å…³è”åº“å­˜ID']
    }
    for col in related_id_cols.get(table_name, []):
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(-1).astype(int)

    return df


# ------------------- å®‰å…¨çš„æ–‡ä»¶æ“ä½œå‡½æ•° -------------------
def safe_read_csv_files():
    """å®‰å…¨åœ°è¯»å–æ‰€æœ‰CSVæ–‡ä»¶ - å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨è¿”å›ç©ºDataFrame"""
    data = {}
    try:
        for table, filepath in CSV_FILES.items():
            if os.path.exists(filepath):
                try:
                    # ä¿®å¤1ï¼šæŒ‡å®šheader=0ç¡®ä¿è¡¨å¤´æ­£ç¡®ï¼Œé¿å…è¯»å–æ—¶ç´¢å¼•åˆ—æ··å…¥æ•°æ®
                    df = pd.read_csv(filepath, header=0, encoding='utf-8-sig')
                    # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºæ–‡ä»¶æˆ–åªæœ‰ç´¢å¼•åˆ—
                    if df.empty or (len(df.columns) == 1 and 'Unnamed: 0' in df.columns):
                        data[table] = get_empty_dataframe_template(table)
                    else:
                        # ä¿®å¤ï¼šæ ‡å‡†åŒ–IDåˆ—ç±»å‹
                        data[table] = normalize_id_columns(df.fillna(""), table)
                    print(f"âœ… æˆåŠŸè¯»å– {table} æ•°æ®ï¼Œè¡Œæ•°: {len(data[table])}")
                except Exception as e:
                    print(f"âš ï¸ è¯»å– {table} æ–‡ä»¶å¤±è´¥: {str(e)}ï¼Œå°è¯•ä»å¤‡ä»½æ¢å¤")
                    # å°è¯•ä»å¤‡ä»½æ¢å¤
                    backup_file = f"{filepath}.backup"
                    if os.path.exists(backup_file):
                        try:
                            shutil.copy2(backup_file, filepath)
                            df = pd.read_csv(filepath, header=0, encoding='utf-8-sig')
                            data[table] = normalize_id_columns(df.fillna(""), table)
                            print(f"ğŸ”„ ä»å¤‡ä»½æ¢å¤ {table} æ•°æ®æˆåŠŸ")
                        except Exception as backup_error:
                            print(f"âŒ å¤‡ä»½æ¢å¤ä¹Ÿå¤±è´¥: {str(backup_error)}ï¼Œä½¿ç”¨ç©ºDataFrame")
                            data[table] = get_empty_dataframe_template(table)
                    else:
                        print(f"ğŸ“ æ— å¤‡ä»½å¯ç”¨ï¼Œä½¿ç”¨ç©ºDataFrame")
                        data[table] = get_empty_dataframe_template(table)
            else:
                print(f"ğŸ“ {table} æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç©ºDataFrame")
                data[table] = get_empty_dataframe_template(table)
        return data
    except Exception as e:
        print(f"âŒ è¯»å–CSVæ–‡ä»¶å¤±è´¥: {str(e)}")
        return {table: get_empty_dataframe_template(table) for table in CSV_FILES.keys()}


def safe_write_csv_files(data, force_override=False):
    """å®‰å…¨å†™å…¥CSVï¼ˆæ”¯æŒå¼ºåˆ¶è¦†ç›–ï¼Œä¸åˆå¹¶ï¼‰"""
    try:
        ensure_csv_directory()
        create_single_backup()  # å…ˆå¤‡ä»½

        write_success = True
        for table, filepath in CSV_FILES.items():
            df = data.get(table, pd.DataFrame())
            # ç»ˆææ¸…ç†ï¼šåˆ é™¤å…¨ç©ºè¡Œã€Unnamedåˆ—
            df = df.dropna(how='all')
            df = df.loc[:, ~df.columns.str.contains('^Unnamed')]

            if force_override:
                # å¼ºåˆ¶è¦†ç›–ï¼šç›´æ¥å†™å…¥ï¼Œä¸åˆå¹¶
                with tempfile.NamedTemporaryFile(mode='w', delete=False, encoding='utf-8-sig') as temp_f:
                    df.to_csv(temp_f, index=False, encoding='utf-8-sig')
                    temp_path = temp_f.name
                if os.path.exists(filepath):
                    os.remove(filepath)
                shutil.move(temp_path, filepath)
            else:
                # åŸæœ‰åˆå¹¶é€»è¾‘ï¼ˆä¿ç•™ï¼‰
                existing_df = safe_read_csv_files().get(table, pd.DataFrame())
                existing_df = existing_df.dropna(how='all')
                existing_df = existing_df.loc[:, ~existing_df.columns.str.contains('^Unnamed')]

                id_col = get_id_column(table)
                if id_col and id_col in df.columns and id_col in existing_df.columns:
                    existing_df = existing_df[~existing_df[id_col].isin(df[id_col])]
                    df = pd.concat([existing_df, df], ignore_index=True).dropna(how='all')

                df.to_csv(filepath, index=False, encoding='utf-8-sig')

            print(f"ğŸ’¾ å†™å…¥ {table}ï¼š{len(df)} è¡Œ")

        invalidate_cache()
        return write_success
    except Exception as e:
        print(f"å†™å…¥å¤±è´¥: {e}")
        restore_from_backup()
        return False


def get_id_column(table_name):
    """è·å–è¡¨çš„ä¸»é”®åˆ—å"""
    id_columns = {
        'product': 'å•†å“ID',
        'feature': 'å•†å“ç‰¹å¾ID',
        'inventory': 'åº“å­˜ID',
        'location': 'åœ°å€ID',
        'operation_record': 'æ“ä½œID',
        'manufacturer': 'å‚å®¶ID',
        'capacity': 'æ¥¼å±‚'  # å®¹é‡è¡¨ä½¿ç”¨æ¥¼å±‚ä½œä¸ºä¸»é”®
    }
    return id_columns.get(table_name, None)


def get_empty_dataframe_template(table_name):
    """è·å–æŒ‡å®šè¡¨çš„ç©ºDataFrameæ¨¡æ¿"""
    templates = {
        'product': pd.DataFrame(columns=[
            "å•†å“ID", "è´§å·", "ç±»å‹", "å¤‡æ³¨", "ç”¨é€”"
        ]),
        'feature': pd.DataFrame(columns=[
            "å•†å“ç‰¹å¾ID", "å…³è”å•†å“ID", "å•ä»·", "é‡é‡", "è§„æ ¼", "æè´¨",
            "é¢œè‰²", "å½¢çŠ¶", "é£æ ¼","å›¾ç‰‡è·¯å¾„"
        ]),
        'inventory': pd.DataFrame(columns=[
            "åº“å­˜ID", "å…³è”å•†å“ç‰¹å¾ID", "å…³è”ä½ç½®ID", "å…³è”å‚å®¶ID",
            "å•ä½", "åº“å­˜æ•°é‡", "æ¬¡å“æ•°é‡", "æ‰¹æ¬¡", "çŠ¶æ€"
        ]),
        'location': pd.DataFrame(columns=[
            "åœ°å€ID", "åœ°å€ç±»å‹", "æ¥¼å±‚", "æ¶å·", "æ¡†å·", "åŒ…å·"
        ]),
        'operation_record': pd.DataFrame(columns=[
            "æ“ä½œID", "å…³è”åº“å­˜ID", "æ“ä½œç±»å‹", "æ“ä½œæ—¶é—´",
            "æ“ä½œæ•°é‡", "æ“ä½œäºº", "å¤‡æ³¨"
        ]),
        'manufacturer': pd.DataFrame(columns=[
            "å‚å®¶ID", "å‚å®¶", "å‚å®¶åœ°å€", "ç”µè¯"
        ]),
        'capacity': pd.DataFrame(columns=[
            "æ¥¼å±‚", "æ¥¼å±‚å®¹é‡", "æ¥¼å±‚å‰©ä½™å®¹é‡"
        ])
    }
    # ä¿®å¤ï¼šç©ºæ¨¡æ¿çš„IDåˆ—é»˜è®¤ç±»å‹ä¸ºæ•´æ•°
    template_df = templates.get(table_name, pd.DataFrame())
    if not template_df.empty:
        id_col = get_id_column(table_name)
        if id_col in template_df.columns:
            template_df[id_col] = template_df[id_col].astype('Int64')  # å¯ç©ºæ•´æ•°ç±»å‹
    return template_df


# ------------------- æ•°æ®åˆå§‹åŒ–å‡½æ•° -------------------
def init_or_fix_csv_files():
    """åˆå§‹åŒ–æˆ–ä¿®å¤CSVæ–‡ä»¶ - å®‰å…¨çš„åˆå§‹åŒ–æµç¨‹"""
    # ç¡®ä¿CSVç›®å½•å­˜åœ¨
    ensure_csv_directory()

    # é¦–å…ˆå°è¯•è¯»å–ç°æœ‰æ•°æ®
    data = safe_read_csv_files()

    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºæˆ–ä¿®å¤æ–‡ä»¶
    needs_creation = False
    needs_repair = False

    for table in REQUIRED_TABLES:
        filepath = CSV_FILES[table]
        if not os.path.exists(filepath):
            print(f"ğŸ“ {table} æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦åˆ›å»º")
            needs_creation = True
        elif table not in data or data[table] is None or data[table].empty:
            print(f"âš ï¸ {table} æ•°æ®ä¸ºç©ºæˆ–æŸåï¼Œéœ€è¦ä¿®å¤")
            needs_repair = True
        else:
            # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
            required_cols = get_required_columns(table)
            if required_cols:
                existing_cols = set(data[table].columns)
                missing_cols = set(required_cols) - existing_cols
                if missing_cols:
                    print(f"âš ï¸ {table} ç¼ºå°‘åˆ—: {missing_cols}ï¼Œéœ€è¦ä¿®å¤")
                    needs_repair = True

    if needs_creation:
        print("ğŸ†• åˆ›å»ºæ–°çš„CSVæ–‡ä»¶")
        result = create_new_csv_files()
        invalidate_cache()
        return result
    elif needs_repair:
        print("ğŸ”§ ä¿®å¤CSVæ–‡ä»¶ç»“æ„")
        try:
            result = repair_csv_structure(data)
            invalidate_cache()
            return result
        except Exception as e:
            print(f"âŒ ä¿®å¤å¤±è´¥: {str(e)}ï¼Œå°è¯•é‡æ–°åˆ›å»º")
            result = create_new_csv_files()
            invalidate_cache()
            return result
    else:
        print("âœ… CSVæ–‡ä»¶çŠ¶æ€æ­£å¸¸")
        return True


def get_required_columns(table_name):
    """è·å–æŒ‡å®šè¡¨å¿…éœ€çš„åˆ—"""
    required_columns = {
        'product': ["å•†å“ID", "è´§å·", "ç±»å‹", "å¤‡æ³¨", "ç”¨é€”"],
        'feature': ["å•†å“ç‰¹å¾ID", "å…³è”å•†å“ID", "å•ä»·", "é‡é‡", "è§„æ ¼", "æè´¨", "é¢œè‰²", "å½¢çŠ¶", "é£æ ¼", "å›¾ç‰‡è·¯å¾„"],
        'inventory': ["åº“å­˜ID", "å…³è”å•†å“ç‰¹å¾ID", "å…³è”ä½ç½®ID", "å…³è”å‚å®¶ID", "å•ä½", "åº“å­˜æ•°é‡", "æ¬¡å“æ•°é‡", "æ‰¹æ¬¡",
                      "çŠ¶æ€"],
        'location': ["åœ°å€ID", "åœ°å€ç±»å‹", "æ¥¼å±‚", "æ¶å·", "æ¡†å·", "åŒ…å·"],
        'operation_record': ["æ“ä½œID", "å…³è”åº“å­˜ID", "æ“ä½œç±»å‹", "æ“ä½œæ—¶é—´", "æ“ä½œæ•°é‡", "æ“ä½œäºº", "å¤‡æ³¨"],
        'manufacturer': ["å‚å®¶ID", "å‚å®¶", "å‚å®¶åœ°å€", "ç”µè¯"],
        'capacity': ["æ¥¼å±‚", "æ¥¼å±‚å®¹é‡", "æ¥¼å±‚å‰©ä½™å®¹é‡"]
    }
    return required_columns.get(table_name, [])


def create_new_csv_files():
    """åˆ›å»ºæ–°çš„CSVæ–‡ä»¶ - å®‰å…¨çš„åˆ›å»ºæµç¨‹"""
    try:
        # ç¡®ä¿CSVç›®å½•å­˜åœ¨
        ensure_csv_directory()

        # è¯»å–ç°æœ‰æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
        existing_data = safe_read_csv_files()

        # åˆ›å»ºæ ‡å‡†æ•°æ®ç»“æ„
        new_data = {
            'product': get_empty_dataframe_template('product'),
            'feature': get_empty_dataframe_template('feature'),
            'inventory': get_empty_dataframe_template('inventory'),
            'location': get_empty_dataframe_template('location'),
            'operation_record': get_empty_dataframe_template('operation_record'),
            'manufacturer': get_empty_dataframe_template('manufacturer'),
            'capacity': get_empty_dataframe_template('capacity')
        }

        # åˆå¹¶ç°æœ‰æ•°æ®å’Œæ–°æ•°æ®ç»“æ„
        merged_data = {}
        for table in REQUIRED_TABLES:
            if table in existing_data and not existing_data[table].empty:
                # ä¿ç•™ç°æœ‰æ•°æ®ï¼Œä½†ç¡®ä¿åˆ—ç»“æ„æ­£ç¡®
                existing_df = existing_data[table]
                template_df = new_data[table]

                # æ·»åŠ ç¼ºå¤±çš„åˆ—
                for col in template_df.columns:
                    if col not in existing_df.columns:
                        if col in ["åº“å­˜æ•°é‡", "æ¬¡å“æ•°é‡", "æ¥¼å±‚"]:
                            existing_df[col] = 0
                        elif col == "çŠ¶æ€":
                            existing_df[col] = "æ­£å¸¸"
                        elif col == "åœ°å€ç±»å‹":
                            existing_df[col] = 1
                        elif col == "æ“ä½œç±»å‹":
                            existing_df[col] = "å…¥åº“"
                        else:
                            existing_df[col] = ""

                # ç§»é™¤å¤šä½™çš„åˆ—
                for col in existing_df.columns:
                    if col not in template_df.columns:
                        existing_df = existing_df.drop(columns=[col])

                # ä¿®å¤ï¼šé‡ç½®ç´¢å¼•
                merged_data[table] = existing_df.reset_index(drop=True)
            else:
                merged_data[table] = new_data[table]

        # å†™å…¥æ–‡ä»¶
        result = safe_write_csv_files(merged_data)
        if result:
            print("âœ… CSVæ–‡ä»¶åˆ›å»º/åˆå§‹åŒ–æˆåŠŸ")
        else:
            print("âŒ CSVæ–‡ä»¶åˆ›å»ºå¤±è´¥")
        return result

    except Exception as e:
        print(f"âŒ åˆ›å»ºCSVæ–‡ä»¶å¤±è´¥: {str(e)}")
        return False


# ------------------- æ•°æ®ä¿®å¤å‡½æ•° -------------------
def repair_csv_structure(data):
    """ä¿®å¤CSVæ•°æ®ç»“æ„ - å®‰å…¨çš„ä¿®å¤æµç¨‹"""
    try:
        # é¦–å…ˆè¯»å–ç°æœ‰æ•°æ®ä½œä¸ºåŸºå‡†
        existing_data = safe_read_csv_files()

        # ç”¨ç°æœ‰æ•°æ®å¡«è¡¥ç¼ºå¤±çš„è¡¨
        for table in REQUIRED_TABLES:
            if table not in data or data[table] is None or data[table].empty:
                if table in existing_data and not existing_data[table].empty:
                    data[table] = existing_data[table]
                else:
                    data[table] = get_empty_dataframe_template(table)
            # ä¿®å¤ï¼šæ ‡å‡†åŒ–IDåˆ—
            data[table] = normalize_id_columns(data[table], table)

        # ä¿®å¤å„ä¸ªè¡¨çš„ç»“æ„
        repair_functions = {
            'product': repair_product_table,
            'feature': repair_feature_table,
            'inventory': repair_inventory_table,
            'location': repair_location_table,
            'operation_record': repair_operation_record_table,
            'manufacturer': repair_manufacturer_table,
            'capacity': repair_capacity_table
        }

        for table, repair_func in repair_functions.items():
            if table in data:
                data[table] = repair_func(data[table], data)
                # ä¿®å¤ï¼šé‡ç½®ç´¢å¼•
                data[table] = data[table].reset_index(drop=True)

        # ä¿å­˜ä¿®å¤åçš„æ•°æ®
        result = safe_write_csv_files(data)

        if result:
            print("âœ… CSVæ–‡ä»¶ä¿®å¤æˆåŠŸ")
        else:
            print("âŒ CSVæ–‡ä»¶ä¿®å¤å¤±è´¥")
        return result

    except Exception as e:
        print(f"âŒ ä¿®å¤CSVç»“æ„å¤±è´¥: {str(e)}")
        return False


def repair_product_table(product_df, all_data):
    """ä¿®å¤å•†å“è¡¨ç»“æ„"""
    required_cols = get_required_columns('product')

    # æ·»åŠ ç¼ºå¤±çš„åˆ—
    for col in required_cols:
        if col not in product_df.columns:
            product_df[col] = ""

    # ä¿®å¤ï¼šé‡ç½®ç´¢å¼•
    return product_df.reset_index(drop=True)


def repair_feature_table(feature_df, all_data):
    """ä¿®å¤å•†å“ç‰¹å¾è¡¨ç»“æ„"""
    required_cols = get_required_columns('feature')

    # æ·»åŠ ç¼ºå¤±çš„åˆ—
    for col in required_cols:
        if col not in feature_df.columns:
            if col in ["å•ä»·", "é‡é‡"]:
                feature_df[col] = 0.0
            else:
                feature_df[col] = ""

    # ç¡®ä¿å…³è”å•†å“IDå­˜åœ¨
    if "å…³è”å•†å“ID" in feature_df.columns and "å•†å“ID" in all_data.get('product', pd.DataFrame()).columns:
        product_ids = set(all_data['product']["å•†å“ID"].tolist())
        feature_df["å…³è”å•†å“ID"] = feature_df["å…³è”å•†å“ID"].apply(
            lambda x: x if x in product_ids else -1
        )

    # ä¿®å¤ï¼šé‡ç½®ç´¢å¼•
    return feature_df.reset_index(drop=True)


def repair_inventory_table(inventory_df, all_data):
    """ä¿®å¤åº“å­˜è¡¨ç»“æ„"""
    required_cols = get_required_columns('inventory')

    # æ·»åŠ ç¼ºå¤±çš„åˆ—
    for col in required_cols:
        if col not in inventory_df.columns:
            if col in ["åº“å­˜æ•°é‡", "æ¬¡å“æ•°é‡", "æ‰¹æ¬¡"]:
                inventory_df[col] = 0
            elif col == "çŠ¶æ€":
                inventory_df[col] = "æ­£å¸¸"
            elif col == "å•ä½":
                inventory_df[col] = "ä¸ª"
            else:
                inventory_df[col] = -1  # å…³è”IDé»˜è®¤-1ï¼ˆæ— æ•ˆï¼‰

    # éªŒè¯å¤–é”®å…³è”
    if "å…³è”å•†å“ç‰¹å¾ID" in inventory_df.columns and "å•†å“ç‰¹å¾ID" in all_data.get('feature', pd.DataFrame()).columns:
        feature_ids = set(all_data['feature']["å•†å“ç‰¹å¾ID"].tolist())
        inventory_df["å…³è”å•†å“ç‰¹å¾ID"] = inventory_df["å…³è”å•†å“ç‰¹å¾ID"].apply(
            lambda x: x if x in feature_ids else -1
        )

    if "å…³è”ä½ç½®ID" in inventory_df.columns and "åœ°å€ID" in all_data.get('location', pd.DataFrame()).columns:
        location_ids = set(all_data['location']["åœ°å€ID"].tolist())
        inventory_df["å…³è”ä½ç½®ID"] = inventory_df["å…³è”ä½ç½®ID"].apply(
            lambda x: x if x in location_ids else -1
        )

    if "å…³è”å‚å®¶ID" in inventory_df.columns and "å‚å®¶ID" in all_data.get('manufacturer', pd.DataFrame()).columns:
        manufacturer_ids = set(all_data['manufacturer']["å‚å®¶ID"].tolist())
        inventory_df["å…³è”å‚å®¶ID"] = inventory_df["å…³è”å‚å®¶ID"].apply(
            lambda x: x if x in manufacturer_ids else -1
        )

    # ä¿®å¤ï¼šé‡ç½®ç´¢å¼•
    return inventory_df.reset_index(drop=True)


def repair_location_table(location_df, all_data):
    """ä¿®å¤ä½ç½®è¡¨ç»“æ„"""
    required_cols = get_required_columns('location')

    # æ·»åŠ ç¼ºå¤±çš„åˆ—
    for col in required_cols:
        if col not in location_df.columns:
            if col in ["åœ°å€ç±»å‹", "æ¥¼å±‚"]:
                location_df[col] = 1
            else:
                location_df[col] = ""

    # ä¿®å¤ï¼šé‡ç½®ç´¢å¼•
    return location_df.reset_index(drop=True)


def repair_operation_record_table(record_df, all_data):
    """ä¿®å¤æ“ä½œè®°å½•è¡¨ç»“æ„"""
    required_cols = get_required_columns('operation_record')

    # æ·»åŠ ç¼ºå¤±çš„åˆ—
    for col in required_cols:
        if col not in record_df.columns:
            if col == "æ“ä½œæ•°é‡":
                record_df[col] = 0
            elif col == "æ“ä½œç±»å‹":
                record_df[col] = "å…¥åº“"
            elif col == "æ“ä½œæ—¶é—´":
                record_df[col] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            else:
                record_df[col] = ""

    # éªŒè¯å¤–é”®å…³è”
    if "å…³è”åº“å­˜ID" in record_df.columns and "åº“å­˜ID" in all_data.get('inventory', pd.DataFrame()).columns:
        inventory_ids = set(all_data['inventory']["åº“å­˜ID"].tolist())
        record_df["å…³è”åº“å­˜ID"] = record_df["å…³è”åº“å­˜ID"].apply(
            lambda x: x if x in inventory_ids else -1
        )

    # ä¿®å¤ï¼šé‡ç½®ç´¢å¼•
    return record_df.reset_index(drop=True)


def repair_manufacturer_table(manufacturer_df, all_data):
    """ä¿®å¤å‚å®¶è¡¨ç»“æ„"""
    required_cols = get_required_columns('manufacturer')

    # æ·»åŠ ç¼ºå¤±çš„åˆ—
    for col in required_cols:
        if col not in manufacturer_df.columns:
            manufacturer_df[col] = ""

    # ä¿®å¤ï¼šé‡ç½®ç´¢å¼•
    return manufacturer_df.reset_index(drop=True)


def repair_capacity_table(capacity_df, all_data):
    """ä¿®å¤å®¹é‡è¡¨ç»“æ„"""
    # ä¿®å¤ï¼šç¡®ä¿æ¥¼å±‚åˆ—æ˜¯æ•´æ•°ç±»å‹
    if "æ¥¼å±‚" in capacity_df.columns:
        capacity_df["æ¥¼å±‚"] = pd.to_numeric(capacity_df["æ¥¼å±‚"], errors='coerce').fillna(0).astype(int)

    # è·å–æ‰€æœ‰ä½¿ç”¨çš„æ¥¼å±‚
    inventory_df = all_data.get('inventory', pd.DataFrame())
    location_df = all_data.get('location', pd.DataFrame())

    # è·å–æ‰€æœ‰å·²ä½¿ç”¨çš„æ¥¼å±‚
    used_floors = set()
    if not inventory_df.empty and "å…³è”ä½ç½®ID" in inventory_df.columns and not location_df.empty:
        # è·å–åº“å­˜è¡¨ä¸­çš„ä½ç½®ID
        location_ids = inventory_df["å…³è”ä½ç½®ID"].dropna().tolist()
        # ä»ä½ç½®è¡¨è·å–å¯¹åº”çš„æ¥¼å±‚
        if "åœ°å€ID" in location_df.columns and "æ¥¼å±‚" in location_df.columns:
            location_df["åœ°å€ID"] = pd.to_numeric(location_df["åœ°å€ID"], errors='coerce').fillna(-1).astype(int)
            location_df["æ¥¼å±‚"] = pd.to_numeric(location_df["æ¥¼å±‚"], errors='coerce').fillna(0).astype(int)
            location_dict = location_df.set_index("åœ°å€ID")["æ¥¼å±‚"].to_dict()
            for loc_id in location_ids:
                if loc_id in location_dict:
                    used_floors.add(int(location_dict[loc_id]))

    # ç¡®ä¿æ¥¼å±‚1-5éƒ½å­˜åœ¨
    for floor in range(1, 6):
        if floor not in capacity_df["æ¥¼å±‚"].values:
            used_boxes = 1 if floor in used_floors else 0
            new_row = pd.DataFrame([{
                "æ¥¼å±‚": floor,
                "æ¥¼å±‚å®¹é‡": FLOOR_CAPACITY if hasattr(globals(), 'FLOOR_CAPACITY') else 100,
                "æ¥¼å±‚å‰©ä½™å®¹é‡": max(0, (FLOOR_CAPACITY if hasattr(globals(), 'FLOOR_CAPACITY') else 100) - used_boxes)
            }])
            capacity_df = pd.concat([capacity_df, new_row], ignore_index=True)

    # ä¿®å¤ï¼šé‡ç½®ç´¢å¼•
    return capacity_df.reset_index(drop=True)


# ------------------- æ ¸å¿ƒæ•°æ®æ“ä½œå‡½æ•° -------------------
def read_csv_data():
    """è¯»å–CSVæ•°æ® - ä¼˜å…ˆä»ç¼“å­˜è¯»å–"""
    return get_cached_csv_data()


def write_csv_data(data):
    """å†™å…¥CSVæ•°æ® - å®‰å…¨çš„å†™å…¥æµç¨‹"""
    return safe_write_csv_files(data)


def add_data_to_csv(new_data_dict):
    """
    å®‰å…¨åœ°æ·»åŠ æ•°æ®åˆ°CSVæ–‡ä»¶
    new_data_dict: {'table_name': DataFrameæˆ–å­—å…¸åˆ—è¡¨, ...}
    """
    try:
        # 1. é¦–å…ˆè¯»å–ç°æœ‰æ•°æ®
        existing_data = read_csv_data()

        # 2. åˆå¹¶æ–°æ•°æ®åˆ°ç°æœ‰æ•°æ®
        for table_name, new_data in new_data_dict.items():
            if table_name in existing_data:
                if isinstance(new_data, pd.DataFrame):
                    new_df = new_data
                else:
                    # å¦‚æœæ˜¯å­—å…¸åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºDataFrame
                    new_df = pd.DataFrame(new_data)

                if not new_df.empty:
                    # ç”Ÿæˆè‡ªå¢IDï¼ˆå¦‚æœéœ€è¦ï¼‰
                    id_column = get_id_column(table_name)
                    if id_column and id_column not in new_df.columns:
                        # è·å–ä¸‹ä¸€ä¸ªID
                        next_id = generate_auto_id_df(existing_data[table_name], id_column)
                        new_df[id_column] = range(next_id, next_id + len(new_df))

                    # åˆå¹¶æ•°æ®
                    if not existing_data[table_name].empty:
                        existing_data[table_name] = pd.concat(
                            [existing_data[table_name], new_df],
                            ignore_index=True
                        )
                    else:
                        existing_data[table_name] = new_df

                    # ä¿®å¤ï¼šé‡ç½®ç´¢å¼•
                    existing_data[table_name] = existing_data[table_name].reset_index(drop=True)
                    print(f"âœ… æˆåŠŸæ·»åŠ  {len(new_df)} è¡Œæ•°æ®åˆ° {table_name}")

        # 3. å†™å…¥åˆå¹¶åçš„æ•°æ®
        result = write_csv_data(existing_data)
        if result:
            print("ğŸ‰ æ•°æ®æ·»åŠ æˆåŠŸ")
        else:
            print("âŒ æ•°æ®æ·»åŠ å¤±è´¥")
        return result

    except Exception as e:
        print(f"âŒ æ·»åŠ æ•°æ®å¤±è´¥: {str(e)}")
        return False


# ------------------- æ•°æ®å¤„ç†å·¥å…·å‡½æ•° -------------------
def generate_auto_id_df(df, id_column="ID"):
    """ä¸ºDataFrameç”Ÿæˆè‡ªå¢ID"""
    if df.empty or id_column not in df.columns or df[id_column].empty:
        return 1

    valid_ids = []
    for id_val in df[id_column].tolist():
        if isinstance(id_val, (int, float)) and not pd.isna(id_val):
            valid_ids.append(int(id_val))

    return max(valid_ids) + 1 if valid_ids else 1


def convert_to_serializable(value):
    """
    æ ¸å¿ƒä¿®æ”¹ï¼šä¼˜å…ˆç”¨ pd.isna æ‹¦æˆªæ‰€æœ‰ç¼ºå¤±å€¼ï¼ˆåŒ…æ‹¬ NaTï¼‰ï¼Œå†å¤„ç†å…¶ä»–ç±»å‹
    å½»åº•é¿å… NaT èµ°åˆ° strftime è¡Œ
    """
    # ========== ç¬¬ä¸€æ­¥ï¼šä¼˜å…ˆæ‹¦æˆª ALL ç¼ºå¤±å€¼ï¼ˆNaT/NaN/None/ç©ºå­—ç¬¦ä¸²ï¼‰ ==========
    # pd.isna èƒ½è¯†åˆ« NaT/NaN/Noneï¼Œæ˜¯æœ€å¯é çš„åˆ¤æ–­æ–¹å¼
    if pd.isna(value) or value == "" or value is None:
        return ""

    # ========== ç¬¬äºŒæ­¥ï¼šå¤„ç†æ—¶é—´ç±»å‹ï¼ˆæ­¤æ—¶å·²æ—  NaTï¼‰ ==========
    # å…¼å®¹ pandas Timestamp + Python åŸç”Ÿ datetime
    if isinstance(value, (pd.Timestamp, datetime)):
        try:
            return value.strftime("%Y-%m-%d %H:%M:%S")
        except (ValueError, TypeError):
            return ""

    # ========== ç¬¬ä¸‰æ­¥ï¼šå¤„ç†æ•°å€¼ç±»å‹ï¼ˆnumpy â†’ Python åŸç”Ÿï¼‰ ==========
    if isinstance(value, (np.integer, np.int64, np.int32, int)):
        return int(value)
    if isinstance(value, (np.floating, np.float64, np.float32, float)):
        return float(value)

    # ========== ç¬¬å››æ­¥ï¼šå¤„ç†å¸ƒå°”ç±»å‹ ==========
    if isinstance(value, (np.bool_, bool)):
        return bool(value)

    # ========== ç¬¬äº”æ­¥ï¼šå…œåº•ï¼ˆç¡®ä¿ä¸ºå­—ç¬¦ä¸²ï¼‰ ==========
    try:
        return str(value) if not isinstance(value, (str, int, float, bool)) else value
    except:
        return ""


def df_to_serializable_list(df):
    """æ‰¹é‡è½¬æ¢DataFrameä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸åˆ—è¡¨"""
    if df.empty:
        return []
    # æ›¿æ¢ applymapï¼šé€åˆ—ï¼ˆSeriesï¼‰è°ƒç”¨ mapï¼Œç­‰ä»·åŸ applymap é€»è¾‘
    df_serialized = df.reset_index(drop=True).apply(
        lambda col: col.map(convert_to_serializable)  # æ¯ä¸€åˆ—ï¼ˆSeriesï¼‰ç”¨ map å¤„ç†æ¯ä¸ªå…ƒç´ 
    )
    return df_serialized.to_dict('records')

def get_unit_by_addr_type(addr_type):
    """æ ¹æ®åœ°å€ç±»å‹è·å–å•ä½"""
    if addr_type == 1:
        return "æ¡†"
    elif addr_type in [2, 3]:
        return "åŒ…"
    elif addr_type in [4, 5, 6]:
        return "ä¸ª"
    else:
        return "ä¸ª"


def get_unique_boxes_by_floor(floor, location_list):
    """è·å–æŒ‡å®šæ¥¼å±‚ä½¿ç”¨çš„å”¯ä¸€æ¡†å·é›†åˆ"""
    used_boxes = set()
    for item in location_list:
        item_floor = item.get("æ¥¼å±‚", 0)
        if item_floor == floor and "æ¡†å·" in item and item["æ¡†å·"] and not pd.isna(item["æ¡†å·"]):
            box_no = str(item["æ¡†å·"]).strip()
            if box_no:
                used_boxes.add(box_no)
    return used_boxes


def update_capacity(floor, location_list, capacity_df):
    """æ›´æ–°æŒ‡å®šæ¥¼å±‚çš„å‰©ä½™å®¹é‡ï¼Œæ— è®°å½•åˆ™åˆ›å»º"""
    # ä¿®å¤ï¼šç¡®ä¿flooræ˜¯æ•´æ•°
    floor = int(floor) if isinstance(floor, (int, float)) else 0
    used_boxes = len(get_unique_boxes_by_floor(floor, location_list))

    # ä¿®å¤ï¼šç¡®ä¿æ¥¼å±‚åˆ—æ˜¯æ•´æ•°
    capacity_df["æ¥¼å±‚"] = pd.to_numeric(capacity_df["æ¥¼å±‚"], errors='coerce').fillna(0).astype(int)
    floor_capacity_rows = capacity_df[capacity_df["æ¥¼å±‚"] == floor]

    floor_capacity_val = FLOOR_CAPACITY if hasattr(globals(), 'FLOOR_CAPACITY') else 100
    if floor_capacity_rows.empty:
        floor_capacity = {
            "æ¥¼å±‚": floor,
            "æ¥¼å±‚å®¹é‡": floor_capacity_val,
            "æ¥¼å±‚å‰©ä½™å®¹é‡": max(0, floor_capacity_val - used_boxes)
        }
        new_row = pd.DataFrame([floor_capacity])
        capacity_df = pd.concat([capacity_df, new_row], ignore_index=True)
        # ä¿®å¤ï¼šé‡ç½®ç´¢å¼•
        capacity_df = capacity_df.reset_index(drop=True)
        return floor_capacity, capacity_df
    else:
        index = floor_capacity_rows.index[0]
        capacity_df.at[index, "æ¥¼å±‚å‰©ä½™å®¹é‡"] = max(0, floor_capacity_val - used_boxes)
        return capacity_df.iloc[index].to_dict(), capacity_df


def update_inventory_status(inventory_id, csv_data):
    """æ›´æ–°åº“å­˜çŠ¶æ€ï¼ˆåŸºäºæ“ä½œè®°å½•ï¼‰"""
    # ä¿®å¤ï¼šç¡®ä¿inventory_idæ˜¯æ•´æ•°
    inventory_id = int(inventory_id) if isinstance(inventory_id, (int, float)) else -1

    inventory_df = csv_data.get("inventory", pd.DataFrame())
    operation_df = csv_data.get("operation_record", pd.DataFrame())

    if inventory_df.empty or "åº“å­˜ID" not in inventory_df.columns:
        return

    # ä¿®å¤ï¼šæ ‡å‡†åŒ–åº“å­˜IDåˆ—
    inventory_df["åº“å­˜ID"] = pd.to_numeric(inventory_df["åº“å­˜ID"], errors='coerce').fillna(-1).astype(int)
    mask = inventory_df["åº“å­˜ID"] == inventory_id

    if not mask.any():
        return

    # è®¡ç®—æ€»å…¥åº“å’Œæ€»å‡ºåº“æ•°é‡
    if not operation_df.empty and "å…³è”åº“å­˜ID" in operation_df.columns and "æ“ä½œç±»å‹" in operation_df.columns and "æ“ä½œæ•°é‡" in operation_df.columns:
        # æ ‡å‡†åŒ–æ“ä½œè®°å½•çš„å…³è”åº“å­˜IDå’Œæ“ä½œæ•°é‡
        operation_df["å…³è”åº“å­˜ID"] = pd.to_numeric(operation_df["å…³è”åº“å­˜ID"], errors='coerce').fillna(-1).astype(int)
        operation_df["æ“ä½œæ•°é‡"] = pd.to_numeric(operation_df["æ“ä½œæ•°é‡"], errors='coerce').fillna(0).astype(int)

        # å…¥åº“æ“ä½œ
        stock_in = operation_df[
            (operation_df["å…³è”åº“å­˜ID"] == inventory_id) &
            (operation_df["æ“ä½œç±»å‹"] == "å…¥åº“")
            ]["æ“ä½œæ•°é‡"].sum()

        # å‡ºåº“æ“ä½œ
        stock_out = operation_df[
            (operation_df["å…³è”åº“å­˜ID"] == inventory_id) &
            (operation_df["æ“ä½œç±»å‹"] == "å‡ºåº“")
            ]["æ“ä½œæ•°é‡"].sum()

        current_stock = stock_in - stock_out

        # æ›´æ–°åº“å­˜æ•°é‡
        inventory_df.loc[mask, "åº“å­˜æ•°é‡"] = current_stock

        # æ›´æ–°çŠ¶æ€
        if current_stock > 0:
            status = "æ­£å¸¸"
        elif current_stock == 0:
            status = "å·²å‡ºåº“"
        else:
            status = "å¼‚å¸¸"

        inventory_df.loc[mask, "çŠ¶æ€"] = status

    # ä¿®å¤ï¼šé‡ç½®ç´¢å¼•
    csv_data["inventory"] = inventory_df.reset_index(drop=True)


# ------------------- ç¼“å­˜ä¼˜åŒ– -------------------
_data_cache = {}
_cache_timestamp = None


def read_csv_data_cached():
    """å¸¦ç¼“å­˜çš„æ•°æ®è¯»å–å‡½æ•°"""
    global _data_cache, _cache_timestamp

    current_time = datetime.now()
    if (_cache_timestamp and
            (current_time - _cache_timestamp).total_seconds() < CACHE_TIMEOUT and
            _data_cache):
        return _data_cache.copy()

    data = read_csv_data()
    _data_cache = data.copy()
    _cache_timestamp = current_time

    return data


def write_csv_data_optimized(csv_data):
    """ä¼˜åŒ–çš„æ•°æ®å†™å…¥å‡½æ•°"""
    try:
        global _data_cache, _cache_timestamp
        _data_cache = {}
        _cache_timestamp = None
        return write_csv_data(csv_data)
    except Exception as e:
        print(f"âŒ æ•°æ®å†™å…¥å¤±è´¥: {str(e)}")
        return False


# ------------------- å…¼å®¹æ€§å‡½æ•°ï¼ˆä¿æŒåŸæœ‰æ¥å£ï¼‰ -------------------
def init_or_fix_excel_file():
    """å…¼å®¹æ€§å‡½æ•°ï¼Œä¿æŒåŸæœ‰æ¥å£"""
    return init_or_fix_csv_files()


def read_excel_data():
    """å…¼å®¹æ€§å‡½æ•°ï¼Œä¿æŒåŸæœ‰æ¥å£"""
    return read_csv_data()


def write_excel_data(data):
    """å…¼å®¹æ€§å‡½æ•°ï¼Œä¿æŒåŸæœ‰æ¥å£"""
    return write_csv_data(data)


def safe_read_excel_file():
    """å…¼å®¹æ€§å‡½æ•°ï¼Œä¿æŒåŸæœ‰æ¥å£"""
    return safe_read_csv_files()


def safe_write_excel_file(data):
    """å…¼å®¹æ€§å‡½æ•°ï¼Œä¿æŒåŸæœ‰æ¥å£"""
    return safe_write_csv_files(data)


# ------------------- å¤‡ä»½ç®¡ç†åŠŸèƒ½ -------------------
def list_backups():
    """åˆ—å‡ºæ‰€æœ‰å¤‡ä»½æ–‡ä»¶"""
    try:
        backup_pattern = os.path.join(CSV_DIR, "*.backup*")
        backup_files = glob.glob(backup_pattern)

        if backup_files:
            print("ğŸ“‹ ç°æœ‰å¤‡ä»½æ–‡ä»¶:")
            for backup_file in backup_files:
                file_size = os.path.getsize(backup_file)
                mod_time = datetime.fromtimestamp(os.path.getmtime(backup_file))
                print(f"  {os.path.basename(backup_file)}")
                print(f"    å¤§å°: {file_size} å­—èŠ‚, ä¿®æ”¹æ—¶é—´: {mod_time}")
        else:
            print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°å¤‡ä»½æ–‡ä»¶")

        return backup_files
    except Exception as e:
        print(f"âŒ åˆ—å‡ºå¤‡ä»½æ–‡ä»¶å¤±è´¥: {str(e)}")
        return []


def cleanup_all_backups():
    """æ¸…ç†æ‰€æœ‰å¤‡ä»½æ–‡ä»¶"""
    try:
        backup_files = list_backups()
        if not backup_files:
            print("â„¹ï¸  æ²¡æœ‰å¤‡ä»½æ–‡ä»¶éœ€è¦æ¸…ç†")
            return True

        confirm = input("âš ï¸  ç¡®å®šè¦åˆ é™¤æ‰€æœ‰å¤‡ä»½æ–‡ä»¶å—ï¼Ÿ(y/N): ")
        if confirm.lower() == 'y':
            deleted_count = 0
            for backup_file in backup_files:
                try:
                    os.remove(backup_file)
                    deleted_count += 1
                    print(f"ğŸ—‘ï¸  åˆ é™¤: {backup_file}")
                except Exception as e:
                    print(f"âš ï¸ åˆ é™¤ {backup_file} å¤±è´¥: {str(e)}")

            print(f"âœ… å·²åˆ é™¤ {deleted_count} ä¸ªå¤‡ä»½æ–‡ä»¶")
            return deleted_count == len(backup_files)
        else:
            print("âŒ æ“ä½œå·²å–æ¶ˆ")
            return False

    except Exception as e:
        print(f"âŒ æ¸…ç†å¤‡ä»½å¤±è´¥: {str(e)}")
        return False